{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we need to do is import the required libraries\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decisiond_tree():\n",
    "    d_tree = {}\n",
    "\n",
    "    def learn(self, training_data, features, target):\n",
    "        # def train(training_data, features, target):\n",
    "      training_data = [item for item in training_data]\n",
    "      vals = [record[features.index(target)] for record in training_data]\n",
    "      frequency = {key:0 for key in [item[-1] for item in training_data]}\n",
    "      idx = features.index(target)\n",
    "      for tup in training_data:\n",
    "          frequency[tup[idx]] += 1\n",
    "      max, major = 0, 0\n",
    "      for key in frequency.keys():\n",
    "          if frequency[key]>max:\n",
    "              max = frequency[key]\n",
    "              major = key\n",
    "\n",
    "      if not training_data:\n",
    "          return major\n",
    "      elif (len(features) - 1) <= 0:\n",
    "          return major\n",
    "      elif vals.count(vals[0]) == len(vals):\n",
    "          return vals[0]\n",
    "      else:\n",
    "          best = features[0]\n",
    "          minimum_gini = 10000;\n",
    "\n",
    "          for attr in features:\n",
    "              new_gini_index = get_gini(features, training_data, attr, target)\n",
    "              if new_gini_index<minimum_gini:\n",
    "                  minimum_gini = new_gini_index\n",
    "                  best = attr\n",
    "          d_tree = {best:{}}\n",
    "\n",
    "          idx = features.index(best)\n",
    "          got_vals = []\n",
    "          for entry in training_data:\n",
    "              if entry[idx] not in got_vals:\n",
    "                  got_vals.append(float(entry[idx]))\n",
    "          for val in got_vals: #get_values(data, features, best):\n",
    "              new_data = [[entry[i] for i in range(0, len(entry)) if i != idx] for entry in training_data if entry[idx] == val]\n",
    "              newAttr = features[:]\n",
    "              newAttr.remove(best)\n",
    "              subtree = self.learn(new_data, newAttr, target)\n",
    "              d_tree[best][val] = subtree\n",
    "      return d_tree\n",
    "      \n",
    "    def classify(self, test_instance, features):\n",
    "        # Followinf node is to make sure that we create an entity as node\n",
    "        # the node will store the name of a node in the tree and its children\n",
    "        class Node():\n",
    "          def __init__(self, val, dictionary):\n",
    "              self.value = val\n",
    "              if (isinstance(dictionary, dict)):\n",
    "                  self.children = dictionary.keys()\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        # create a copy of d_tree in the varable temp_dict\n",
    "        temp_dict = self.d_tree.copy()\n",
    "\n",
    "        # Traverse the tree untill reaching the leaf node\n",
    "        while(isinstance(temp_dict, dict)):\n",
    "          # get to the root node of the currect subtree\n",
    "          root = Node(list(temp_dict.keys())[0], temp_dict[list(temp_dict.keys())[0]])\n",
    "          # Store the children of the current node in the temp_dict variable\n",
    "          temp_dict = temp_dict[list(temp_dict.keys())[0]]\n",
    "          # store the idx of the attribute of the current node and store it in idx variable\n",
    "          idx = features.index(root.value)\n",
    "          # Pick the value from the test tuple for the variable location stored in idx variable\n",
    "          value = test_instance[idx]\n",
    "          # store the keys of the \n",
    "          if value in temp_dict.keys():\n",
    "            child = Node(value, temp_dict[value])\n",
    "            result = temp_dict[value]\n",
    "            temp_dict = temp_dict[value]\n",
    "          else:\n",
    "            result = None\n",
    "            break\n",
    "        return result\n",
    "\n",
    "    def gini_target(self, features, data, target_attributes):\n",
    "        # Set up a dictionary to store the no of occurences of the target attribute\n",
    "        freq = {}\n",
    "        # Set the sum of probabilitys\n",
    "        sum_p2 = 0.0\n",
    "        i = features.index(target_attributes) - 1\n",
    "        for entry in data:\n",
    "            if (entry[i] in freq.keys()):\n",
    "                freq[entry[i]] += 1.0\n",
    "            else:\n",
    "                freq[entry[i]]  = 1.0\n",
    "        for freq in freq.values():\n",
    "            sum_p2 += (freq/len(data))**2\n",
    "        gini = 1 - sum_p2\n",
    "        return gini\n",
    "\n",
    "    def get_gini(self, features, data, attr, target_attributes):\n",
    "        # This function will help to calculate the Gini index of the part of data\n",
    "        # Using this function we will calculate the Gini index for each split made by each attribute\n",
    "        var_occur_n = {}\n",
    "\n",
    "        # Subset entropy variable will contain the sum of entropy of each and every split of the attribute\n",
    "        gini_sum = 0.0\n",
    "        # store the idx of the current attribute column\n",
    "        i = features.index(attr)\n",
    "\n",
    "        # Iterating over the the data provided we will canculate the frequency of occurence of \n",
    "        # the unique elements of the attribute\n",
    "        for entry in data:\n",
    "            # for each entry in the data update their frequency\n",
    "            # If the entry is listed as a key then increment the frequency\n",
    "            # On the other hand if the entry is not listed then add it as a new element\n",
    "            if (entry[i] in var_occur_n.keys()):\n",
    "                var_occur_n[entry[i]] += 1.0\n",
    "            else:\n",
    "                var_occur_n[entry[i]]  = 1.0\n",
    "\n",
    "        # For each key in the dictionary canculate the entropy of corresponding subset of data\n",
    "        # then add those entropy values in the subset_entropy\n",
    "        for val in var_occur_n.keys():\n",
    "            Probability_val        = var_occur_n[val] / sum(var_occur_n.values())\n",
    "            dataSubset     = [entry for entry in data if entry[i] == val]\n",
    "            gini_sum += Probability_val * gini_target(features, dataSubset, target_attributes)\n",
    "\n",
    "        return gini_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_d_tree():\n",
    "    \n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "      data = [line for line in csv.reader(f, delimiter=\",\")]\n",
    "      # for line in csv.reader(f, delimiter=\",\"):\n",
    "      #   try:\n",
    "      #       data.append(tuple(float(item) for item in line))\n",
    "      #   except:\n",
    "      #       data.append(line)\n",
    "    # Filtering out the first row because it has the colums names\n",
    "    # These names are not use to our calculation\n",
    "    data = data[1:]\n",
    "    data = [tuple(map(float, rows)) for rows in data]\n",
    "    print(\"Number of records: %d\" % len(data))\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    features = list(range(len(data[0])))\n",
    "    # target containes the idx of the target variable\n",
    "    target = features[-1]\n",
    "    K = 10\n",
    "    # Initialising the acc variable which will contain the accuracy for each fold\n",
    "    # We will use the acc list to get the final average accuracy\n",
    "    acc = []\n",
    "    for k in range(K):\n",
    "      # For each iterations of the total 10 folds the data will be shuffled\n",
    "      random.shuffle(data)\n",
    "      # Now that shuffled data will will be split into training and test sets\n",
    "      training_data = [x for i, x in enumerate(data) if i % K != k]\n",
    "      test_set = [x for i, x in enumerate(data) if i % K == k]\n",
    "\n",
    "      # Now we will initialize the decision tree object\n",
    "      d_tree = Decisiond_tree()\n",
    "      # The object created in above step is now going to be trained with tha data\n",
    "      d_tree.d_tree = d_tree.learn( training_data, features, target )\n",
    "      # lets initialize a list named as results this variable will contain the resuts \n",
    "      # from all the test cases\n",
    "      results = []\n",
    "      # Iterate through all the instances of the test set\n",
    "      for instance in test_set:\n",
    "        result = d_tree.classify( instance[:-1] , features)\n",
    "        if result != None:\n",
    "            results.append(result == instance[-1])\n",
    "\n",
    "      # Accuracy\n",
    "      accuracy = float(results.count(True))/float(len(results))\n",
    "      acc.append(accuracy)\n",
    "    # The avg accuracy is stored in avg_acc variable\n",
    "    avg_acc = sum(acc)/len(acc)\n",
    "    print(\"Average accuracy: %.4f\" % avg_acc)\n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_d_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size:  4521\n",
      "Training size:  4000\n",
      "Test set size:  500\n",
      "Training samples: 1000, ACCURACY: 0.8960\n",
      "Training samples: 2000, ACCURACY: 0.9020\n",
      "Training samples: 4000, ACCURACY: 0.9020\n",
      "--- Running time: 0.401111 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Exetremely Fast Decision Tree i.e. Hoeffding Any Time Tree, described in\n",
    "# \"Extremely Fast Decision Tree\" (Manapragada, Webb & Salehi, 2018)\n",
    "#\n",
    "# this program contains 2 classes: Efdt, EfdtNode\n",
    "# changed to CART: Gini index\n",
    "#\n",
    "# Jamie\n",
    "# 06/06/2018\n",
    "#\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# EFDT node class\n",
    "class EfdtNode:\n",
    "    def __init__(self, possible_split_features):\n",
    "        \"\"\"\n",
    "        nijk: statistics of feature i, value j, class\n",
    "        possible_split_features: features list\n",
    "        \"\"\"\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.split_g = None\n",
    "        self.new_examples_seen = 0\n",
    "        self.total_examples_seen = 0\n",
    "        self.class_frequency = {}\n",
    "        self.nijk = {i: {} for i in possible_split_features}\n",
    "        self.possible_split_features = possible_split_features\n",
    "\n",
    "    def add_children(self, left, right, split_feature, split_value):\n",
    "        self.left_child = left\n",
    "        self.right_child = right\n",
    "        left.parent = self\n",
    "        right.parent = self\n",
    "        self.nijk = {i: {} for i in self.nijk.keys()}\n",
    "\n",
    "        if isinstance(split_value, list):\n",
    "            left_value = split_value[0]\n",
    "            right_value = split_value[1]\n",
    "            if len(left_value) <= 1:\n",
    "                new_features = [None if f == split_feature else f for f in left.possible_split_features]\n",
    "                left.possible_split_features = new_features\n",
    "            if len(right_value) <= 1:\n",
    "                new_features = [None if f == split_feature else f for f in right.possible_split_features]\n",
    "                right.possible_split_features = new_features\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left_child is None and self.right_child is None\n",
    "\n",
    "    # update node stats in order to calculate Gini\n",
    "    def update_stats(self, x, y):\n",
    "        nijk = self.nijk\n",
    "        feats = self.possible_split_features\n",
    "        iterator = [f for f in feats if f is not None]\n",
    "        for i in iterator:\n",
    "            value = x[feats.index(i)]\n",
    "\n",
    "            if value not in nijk[i]:\n",
    "                nijk[i][value] = {y: 1}\n",
    "            else:\n",
    "                try:\n",
    "                    nijk[i][value][y] += 1\n",
    "                except KeyError:\n",
    "                    nijk[i][value][y] = 1\n",
    "\n",
    "        self.total_examples_seen += 1\n",
    "        self.new_examples_seen += 1\n",
    "        class_frequency = self.class_frequency\n",
    "        try:\n",
    "            class_frequency[y] += 1\n",
    "        except KeyError:\n",
    "            class_frequency[y] = 1\n",
    "\n",
    "    # the most frequent classification\n",
    "    def most_frequent(self):\n",
    "        if self.class_frequency:\n",
    "            prediction = max(self.class_frequency, key=self.class_frequency.get)\n",
    "        else:\n",
    "            # if self.class_frequency dict is empty, go back to parent\n",
    "            class_frequency = self.parent.class_frequency\n",
    "            prediction = max(class_frequency, key=class_frequency.get)\n",
    "        return prediction\n",
    "\n",
    "    def check_not_splitting(self, class_frequency):\n",
    "        # compute Gini index for not splitting\n",
    "        g_d1 = 1\n",
    "        # g_d2 = 1\n",
    "        # most = max(class_frequency, key=class_frequency.get)\n",
    "        n = sum(class_frequency.values())\n",
    "        for j, k in class_frequency.items():\n",
    "            g_d1 -= (k/n)**2\n",
    "        return g_d1\n",
    "\n",
    "    def node_split(self, split_feature, split_value):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_value = split_value\n",
    "        features = self.possible_split_features\n",
    "\n",
    "        left = EfdtNode(features)\n",
    "        right = EfdtNode(features)\n",
    "        self.add_children(left, right, split_feature, split_value)\n",
    "\n",
    "    # recursively trace down the tree\n",
    "    def sort_example(self, x, y, delta, nmin, tau):\n",
    "        self.update_stats(x, y)\n",
    "        self.re_evaluate_split(delta, nmin, tau)\n",
    "        if self.is_leaf():\n",
    "            self.attempt_split(delta, nmin, tau)\n",
    "            return\n",
    "        else:\n",
    "            left = self.left_child\n",
    "            right = self.right_child\n",
    "\n",
    "            index = self.possible_split_features.index(self.split_feature)\n",
    "            value = x[index]\n",
    "            split_value = self.split_value\n",
    "\n",
    "            if isinstance(split_value, list):  # discrete value\n",
    "                if value in split_value[0]:\n",
    "                    return left.sort_example(x, y, delta, nmin, tau)\n",
    "                else:\n",
    "                    return right.sort_example(x, y, delta, nmin, tau)\n",
    "            else:  # continuous value\n",
    "                if value <= split_value:\n",
    "                    return left.sort_example(x, y, delta, nmin, tau)\n",
    "                else:\n",
    "                    return right.sort_example(x, y, delta, nmin, tau)\n",
    "\n",
    "    def sort_to_predict(self, x):\n",
    "        if self.is_leaf():\n",
    "            return self\n",
    "        else:\n",
    "            index = self.possible_split_features.index(self.split_feature)\n",
    "            value = x[index]\n",
    "            split_value = self.split_value\n",
    "            if isinstance(split_value, list):  # discrete value\n",
    "                if value in split_value[0]:\n",
    "                    return self.left_child.sort_to_predict(x)\n",
    "                else:\n",
    "                    return self.right_child.sort_to_predict(x)\n",
    "            else:  # continuous value\n",
    "                if value <= split_value:\n",
    "                    return self.left_child.sort_to_predict(x)\n",
    "                else:\n",
    "                    return self.right_child.sort_to_predict(x)\n",
    "\n",
    "    # test node split, return the split feature\n",
    "    def attempt_split(self, delta, nmin, tau):\n",
    "        if self.new_examples_seen < nmin:\n",
    "            return\n",
    "        class_frequency = self.class_frequency\n",
    "        if len(class_frequency) <= 1:\n",
    "            return\n",
    "\n",
    "        self.new_examples_seen = 0  # reset\n",
    "        nijk = self.nijk\n",
    "        g_Xa = 1  # minimum g\n",
    "\n",
    "        Xa = ''\n",
    "        split_value = None\n",
    "        for feature in self.possible_split_features:\n",
    "            if feature is not None:\n",
    "                njk = nijk[feature]\n",
    "                gini, value = self.gini(njk, class_frequency)\n",
    "                if gini < g_Xa:\n",
    "                    g_Xa = gini\n",
    "                    Xa = feature\n",
    "                    split_value = value\n",
    "\n",
    "        epsilon = self.hoeffding_bound(delta)\n",
    "        g_X0 = self.check_not_splitting(class_frequency)\n",
    "        g_Xb = g_X0\n",
    "        if g_Xa < g_X0:\n",
    "            if g_Xb - g_Xa > epsilon:\n",
    "                self.split_g = g_Xa  # split on feature Xa\n",
    "                # print('1 node split')\n",
    "                self.node_split(Xa, split_value)\n",
    "            elif g_Xb - g_Xa < epsilon < tau:\n",
    "                self.split_g = g_Xa  # split on feature Xa\n",
    "                # print('2 node split')\n",
    "                self.node_split(Xa, split_value)\n",
    "\n",
    "    def re_evaluate_split(self, delta, nmin, tau):\n",
    "        if self.new_examples_seen < nmin or self.is_leaf():  # only re-evaluate non-leaf\n",
    "            return\n",
    "        class_frequency = self.class_frequency\n",
    "        if len(class_frequency) <= 1:\n",
    "            return\n",
    "\n",
    "        self.new_examples_seen = 0  # reset\n",
    "        nijk = self.nijk\n",
    "        g_Xa = 1\n",
    "        Xa = ''\n",
    "        split_value = None\n",
    "        for feature in self.possible_split_features:\n",
    "            if feature is not None:\n",
    "                njk = nijk[feature]\n",
    "                gini, value = self.gini(njk, class_frequency)\n",
    "                if gini < g_Xa:\n",
    "                    g_Xa = gini\n",
    "                    Xa = feature\n",
    "                    split_value = value\n",
    "\n",
    "        epsilon = self.hoeffding_bound(delta)\n",
    "        g_X0 = self.check_not_splitting(class_frequency)\n",
    "        split_g = self.split_g  # gini of current split feature\n",
    "\n",
    "        if g_X0 < g_Xa:  # not split\n",
    "            print('kill subtree')\n",
    "            self.kill_subtree()\n",
    "        if split_g - g_Xa > epsilon or split_g - g_Xa < epsilon < tau:\n",
    "            if Xa != self.split_feature:\n",
    "                # print('split on new feature')\n",
    "                self.split_g = g_Xa  # split on feature Xa\n",
    "                self.node_split(Xa, split_value)\n",
    "\n",
    "    def kill_subtree(self):\n",
    "        if not self.is_leaf():\n",
    "            self.left_child = None\n",
    "            self.right_child = None\n",
    "            self.split_feature = None\n",
    "            self.split_value = None\n",
    "            self.split_g = None\n",
    "\n",
    "    def hoeffding_bound(self, delta):\n",
    "        n = self.total_examples_seen\n",
    "        R = np.log(len(self.class_frequency))\n",
    "        return (R * R * np.log(1/delta) / (2 * n))**0.5\n",
    "\n",
    "    def gini(self, njk, class_frequency):\n",
    "        # Gini(D) = 1 - Sum(pi^2)\n",
    "        # Gini(D, F=f) = |D1|/|D|*Gini(D1) + |D2|/|D|*Gini(D2)\n",
    "        D = self.total_examples_seen\n",
    "        m1 = 1  # minimum gini\n",
    "        # m2 = 1  # second minimum gini\n",
    "        Xa_value = None\n",
    "        feature_values = list(njk.keys())  # list() is essential\n",
    "        if not isinstance(feature_values[0], str):  # numeric feature values\n",
    "            sort = np.array(sorted(feature_values))\n",
    "            split = (sort[0:-1] + sort[1:])/2   # vectorized computation, like in R\n",
    "\n",
    "            D1_class_frequency = {j:0 for j in class_frequency.keys()}\n",
    "            for index in range(len(split)):\n",
    "                nk = njk[sort[index]]\n",
    "\n",
    "                for j in nk:\n",
    "                    D1_class_frequency[j] += nk[j]\n",
    "                D1 = sum(D1_class_frequency.values())\n",
    "                D2 = D - D1\n",
    "                g_d1 = 1\n",
    "                g_d2 = 1\n",
    "\n",
    "                D2_class_frequency = {}\n",
    "                for key, value in class_frequency.items():\n",
    "                    if key in D1_class_frequency:\n",
    "                        D2_class_frequency[key] = value - D1_class_frequency[key]\n",
    "                    else:\n",
    "                        D2_class_frequency[key] = value\n",
    "\n",
    "                for key, v in D1_class_frequency.items():\n",
    "                    g_d1 -= (v/D1)**2\n",
    "                for key, v in D2_class_frequency.items():\n",
    "                    g_d2 -= (v/D2)**2\n",
    "                g = g_d1*D1/D + g_d2*D2/D\n",
    "                if g < m1:\n",
    "                    m1 = g\n",
    "                    Xa_value = split[index]\n",
    "                # elif m1 < g < m2:\n",
    "                    # m2 = g\n",
    "\n",
    "            return [m1, Xa_value]\n",
    "\n",
    "        else:  # discrete feature_values\n",
    "            length = len(njk)\n",
    "            if length > 9:  # too many discrete feature values, estimate\n",
    "                for j, k in njk.items():\n",
    "                    D1 = sum(k.values())\n",
    "                    D2 = D - D1\n",
    "                    g_d1 = 1\n",
    "                    g_d2 = 1\n",
    "\n",
    "                    D2_class_frequency = {}\n",
    "                    for key, value in class_frequency.items():\n",
    "                        if key in k:\n",
    "                            D2_class_frequency[key] = value - k[key]\n",
    "                        else:\n",
    "                            D2_class_frequency[key] = value\n",
    "                    for key, v in k.items():\n",
    "                        g_d1 -= (v/D1)**2\n",
    "\n",
    "                    if D2 != 0:\n",
    "                        for key, v in D2_class_frequency.items():\n",
    "                            g_d2 -= (v/D2)**2\n",
    "                    g = g_d1*D1/D + g_d2*D2/D\n",
    "\n",
    "                    if g < m1:\n",
    "                        m1 = g\n",
    "                        Xa_value = j\n",
    "                    # elif m1 < g < m2:\n",
    "                        # m2 = g\n",
    "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
    "\n",
    "            else:  # fewer discrete feature values, get combinations\n",
    "                comb = self.select_combinations(feature_values)\n",
    "                for i in comb:\n",
    "                    left = list(i)\n",
    "                    D1_class_frequency = {key: 0 for key in class_frequency.keys()}\n",
    "                    D2_class_frequency = {key: 0 for key in class_frequency.keys()}\n",
    "                    for j,k in njk.items():\n",
    "                        for key, value in class_frequency.items():\n",
    "                            if j in left:\n",
    "                                if key in k:\n",
    "                                    D1_class_frequency[key] += k[key]\n",
    "                            else:\n",
    "                                if key in k:\n",
    "                                    D2_class_frequency[key] += k[key]\n",
    "                    g_d1 = 1\n",
    "                    g_d2 = 1\n",
    "                    D1 = sum(D1_class_frequency.values())\n",
    "                    D2 = D - D1\n",
    "                    for key1, v1 in D1_class_frequency.items():\n",
    "                        g_d1 -= (v1/D1)**2\n",
    "                    for key2, v2 in D2_class_frequency.items():\n",
    "                        g_d2 -= (v2/D2)**2\n",
    "                    g = g_d1*D1/D + g_d2*D2/D\n",
    "\n",
    "                    if g < m1:\n",
    "                        m1 = g\n",
    "                        Xa_value = left\n",
    "                    # elif m1 < g < m2:\n",
    "                        # m2 = g\n",
    "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
    "            return [m1, [Xa_value, right]]\n",
    "\n",
    "    # divide values into two groups, return the combination of left groups\n",
    "    @staticmethod\n",
    "    def select_combinations(feature_values):\n",
    "        combination = []\n",
    "        e = len(feature_values)\n",
    "        if e % 2 == 0:\n",
    "            end = int(e/2)\n",
    "            for i in range(1, end+1):\n",
    "                if i == end:\n",
    "                    cmb = list(combinations(feature_values, i))\n",
    "                    enough = int(len(cmb)/2)\n",
    "                    combination.extend(cmb[:enough])\n",
    "                else:\n",
    "                    combination.extend(combinations(feature_values, i))\n",
    "        else:\n",
    "            end = int((e-1)/2)\n",
    "            for i in range(1, end+1):\n",
    "                combination.extend(combinations(feature_values, i))\n",
    "        return combination\n",
    "\n",
    "\n",
    "class Efdt:\n",
    "    def __init__(self, features, delta=0.05, nmin=50, tau=0.1):\n",
    "        \"\"\"\n",
    "        :features: list of data features\n",
    "        :delta: used to compute hoeffding bound, error rate\n",
    "        :nmin: to limit the G computations\n",
    "        :tau: to deal with ties\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.delta = delta\n",
    "        self.nmin = nmin\n",
    "        self.tau = tau\n",
    "        self.root = EfdtNode(features)\n",
    "        self.n_examples_processed = 0\n",
    "\n",
    "    # find the path of example\n",
    "    def find_path(self, leaf):\n",
    "        path = []\n",
    "        node = leaf\n",
    "        while node:\n",
    "            path.append(node)\n",
    "            node = node.parent\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    # update the tree by adding training example\n",
    "    def update(self, x, y):\n",
    "        self.n_examples_processed += 1\n",
    "        self.root.sort_example(x, y, self.delta, self.nmin, self.tau)\n",
    "\n",
    "    # predict test example's classification\n",
    "    def predict(self, x_test):\n",
    "        prediction = []\n",
    "        if isinstance(x_test, np.ndarray) or isinstance(x_test, list):\n",
    "            for x in x_test:\n",
    "                leaf = self.root.sort_to_predict(x)\n",
    "                prediction.append(leaf.most_frequent())\n",
    "            return prediction\n",
    "        else:\n",
    "            leaf = self.root.sort_to_predict(x_test)\n",
    "            return leaf.most_frequent()\n",
    "\n",
    "    def print_tree(self, node):\n",
    "        if node.is_leaf():\n",
    "            print('Leaf')\n",
    "        else:\n",
    "            print(node.split_feature)\n",
    "            self.print_tree(node.left_child)\n",
    "            self.print_tree(node.right_child)\n",
    "\n",
    "\n",
    "def test_run():\n",
    "    start_time = time.time()\n",
    "    # bank.csv whole data size: 4521 # skiprows=1, nrows=n\n",
    "    df = pd.read_csv('./dataset/bank.csv', header=0, sep=';')\n",
    "    # df = pd.read_csv('./dataset/default_of_credit_card_clients.csv', skiprows=1, header=0)\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # shuffle data rows\n",
    "    title = list(df.columns.values)\n",
    "    features = title[:-1]\n",
    "    rows = df.shape[0]\n",
    "\n",
    "    ''' change month string to int '''\n",
    "    def month_str_to_int(df1):\n",
    "        import calendar\n",
    "        d = dict((v.lower(),k) for k,v in enumerate(calendar.month_abbr))\n",
    "        df1.month = df1.month.map(d)\n",
    "    month_str_to_int(df)\n",
    "\n",
    "    # convert df to data examples\n",
    "    training_size = 4000\n",
    "    array = df.head(training_size).values\n",
    "    set1 = array[:1000, :]\n",
    "    set2 = array[1000:2000, :]\n",
    "    set3 = array[2000:, :]\n",
    "\n",
    "    # to simulate continuous training, modify the tree for each training set\n",
    "    examples = [set1, set2, set3]\n",
    "\n",
    "    # test set is different from training set\n",
    "    n_test = 500\n",
    "    test_set = df.tail(n_test).values\n",
    "    x_test = test_set[:, :-1]\n",
    "    y_test = test_set[:, -1]\n",
    "\n",
    "    # Heoffding bound (epsilon) parameter delta: with 1 - delta probability\n",
    "    # the true mean is at least r_bar - epsilon\n",
    "    # Efdt parameter nmin: test split if new sample size > nmin\n",
    "    # feature_values: unique values in every feature\n",
    "    # tie breaking: when difference is so small, split when diff_g < epsilon < tau\n",
    "    tree = Efdt(features, delta=0.01, nmin=100, tau=0.5)\n",
    "    print('Total data size: ', rows)\n",
    "    print('Training size: ', training_size)\n",
    "    print('Test set size: ', n_test)\n",
    "    n = 0\n",
    "    for training_set in examples:\n",
    "        n += len(training_set)\n",
    "        x_train = training_set[:, :-1]\n",
    "        y_train = training_set[:, -1]\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            tree.update(x, y)\n",
    "        y_pred = tree.predict(x_test)\n",
    "        print('Training samples:', n, end=', ')\n",
    "        print('ACCURACY: %.4f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"--- Running time: %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size:  4521\n",
      "Test set (tail):  500\n",
      "Training set: 1000, ACCURACY: 0.8840\n",
      "Training set: 2000, ACCURACY: 0.8840\n",
      "Training set: 4000, ACCURACY: 0.8860\n",
      "poutcome\n",
      "Leaf\n",
      "poutcome\n",
      "Leaf\n",
      "contact\n",
      "Leaf\n",
      "education\n",
      "Leaf\n",
      "marital\n",
      "Leaf\n",
      "education\n",
      "Leaf\n",
      "duration\n",
      "duration\n",
      "duration\n",
      "contact\n",
      "Leaf\n",
      "Leaf\n",
      "poutcome\n",
      "Leaf\n",
      "Leaf\n",
      "Leaf\n",
      "Leaf\n",
      "--- Running time: 0.088289 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Very Fast Decision Tree i.e. Hoeffding Tree, described in\n",
    "# \"Mining High-Speed Data Streams\" (Domingos & Hulten, 2000)\n",
    "#\n",
    "# this program contains 2 classes: vfdt, vfdt_node\n",
    "# test in command line window: python3 vfdt.py\n",
    "# changed to CART: Gini index\n",
    "#\n",
    "# Jamie\n",
    "# 02/06/2018\n",
    "# ver 0.03\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# VFDT node class\n",
    "class vfdt_node:\n",
    "    # parameter nijk: statistics of feature i, value j, class k\n",
    "    def __init__(self, possible_split_features):\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.new_examples_seen = 0\n",
    "        self.total_examples_seen = 0\n",
    "        self.class_frequency = {}\n",
    "        self.nijk = {i:{} for i in possible_split_features}\n",
    "        self.possible_split_features = possible_split_features\n",
    "\n",
    "    def add_children(self, split_feature, left, right):\n",
    "        self.split_feature = split_feature\n",
    "        self.left_child = left\n",
    "        self.right_child = right\n",
    "        left.parent = self\n",
    "        right.parent = self\n",
    "        self.nijk.clear()  # reset stats\n",
    "\n",
    "    # recursively trace down the tree to distribute data examples to corresponding leaves\n",
    "    def sort_example(self, example):\n",
    "        if (not self.is_leaf()):\n",
    "            index = self.possible_split_features.index(self.split_feature)\n",
    "            value = example[:-1][index]\n",
    "\n",
    "            try:  # continous value\n",
    "                if value <= self.split_value:\n",
    "                    return self.left_child.sort_example(example)\n",
    "                else:\n",
    "                    return self.right_child.sort_example(example)\n",
    "            except TypeError:  # discrete value\n",
    "                if value in self.split_value[0]:\n",
    "                    return self.left_child.sort_example(example)\n",
    "                else:\n",
    "                    return self.right_child.sort_example(example)\n",
    "        else:\n",
    "            return(self)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return(self.left_child == None and self.right_child == None)\n",
    "\n",
    "    # the most frequent classification\n",
    "    def most_frequent(self):\n",
    "        if (self.class_frequency):\n",
    "            prediction = max(self.class_frequency, key=self.class_frequency.get)\n",
    "        else:\n",
    "            # if self.class_frequency dict is empty, go back to parent\n",
    "            class_frequency = self.parent.class_frequency\n",
    "            prediction = max(class_frequency, key=class_frequency.get)\n",
    "        return(prediction)\n",
    "\n",
    "    # upadate leaf stats in order to calculate infomation gain\n",
    "    def update_stats(self, example):\n",
    "        label = example[-1]\n",
    "        feats = self.possible_split_features\n",
    "        for i in feats:\n",
    "            if (i != None):\n",
    "                value = example[:-1][feats.index(i)]\n",
    "                if (value not in self.nijk[i]):\n",
    "                    d = {label : 1}\n",
    "                    self.nijk[i][value] = d\n",
    "                else:\n",
    "                    if (label not in self.nijk[i][value]):\n",
    "                        self.nijk[i][value][label] = 1\n",
    "                    else:\n",
    "                        self.nijk[i][value][label] += 1\n",
    "        self.total_examples_seen += 1\n",
    "        self.new_examples_seen += 1\n",
    "        if (label not in self.class_frequency):\n",
    "            self.class_frequency[label] = 1\n",
    "        else:\n",
    "            self.class_frequency[label] += 1\n",
    "\n",
    "    def check_not_splitting(self):\n",
    "        # compute Gini index for not splitting\n",
    "        X0 = 1\n",
    "        class_frequency = self.class_frequency\n",
    "        n = sum(class_frequency.values())\n",
    "        for j, k in class_frequency.items():\n",
    "            X0 -= (k/n)**2\n",
    "        return X0\n",
    "\n",
    "    # use hoeffding tree model to test node split, return the split feature\n",
    "    def splittable(self, delta, nmin, tau):\n",
    "        if(self.new_examples_seen < nmin):\n",
    "            return(None)\n",
    "        else:\n",
    "            self.new_examples_seen = 0  # reset\n",
    "        nijk = self.nijk\n",
    "        min = 1\n",
    "        second_min = 1\n",
    "        Xa = ''\n",
    "        split_value = None\n",
    "        for feature in self.possible_split_features:\n",
    "            if (len(nijk[feature]) != 1):\n",
    "                gini, value = self.Gini(feature)\n",
    "                if (gini < min):\n",
    "                    min = gini\n",
    "                    Xa = feature\n",
    "                    split_value = value\n",
    "                elif (min < gini < second_min):\n",
    "                    second_min = gini\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        sigma = self.hoeffding_bound(delta)\n",
    "        X0 = self.check_not_splitting()\n",
    "        if min < X0:\n",
    "            if (second_min - min > sigma):\n",
    "                return [Xa, split_value]\n",
    "            elif (sigma < tau and second_min - min < tau):\n",
    "                return [Xa, split_value]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def hoeffding_bound(self, delta):\n",
    "        n = self.total_examples_seen\n",
    "        R = np.log(len(self.class_frequency))\n",
    "        return (R * R * np.log(1/delta) / (2 * n))**0.5\n",
    "\n",
    "    def Gini(self, feature):\n",
    "        '''\n",
    "        Gini(D) = 1 - Sum(pi^2)\n",
    "        Gini(D, F=f) = |D1|/|D|*Gini(D1) + |D2|/|D|*Gini(D2)\n",
    "        '''\n",
    "        njk = self.nijk[feature]\n",
    "        D = self.total_examples_seen\n",
    "        class_frequency = self.class_frequency\n",
    "\n",
    "        m1 = 1  # minimum gini\n",
    "        m2 = 1  # second minimum gini\n",
    "        Xa_value = None\n",
    "        test = next(iter(njk))  # test j value\n",
    "        # if not isinstance(test, np.object):\n",
    "        try:  # continous feature values\n",
    "            test += 0\n",
    "            sort = sorted([j for j in njk.keys()])\n",
    "            split = []\n",
    "            for i in range(1, len(sort)):\n",
    "                temp = (sort[i-1] + sort[i])/2\n",
    "                split.append(temp)\n",
    "\n",
    "            D1 = 0\n",
    "            D1_class_frequency = {j:0 for j in class_frequency.keys()}\n",
    "            for index in range(len(split)):\n",
    "                nk = njk[sort[index]]\n",
    "\n",
    "                for j in nk:\n",
    "                    D1_class_frequency[j] += nk[j]\n",
    "                D1 += sum(nk.values())\n",
    "                D2 = D - D1\n",
    "                g_d1 = 1\n",
    "                g_d2 = 1\n",
    "\n",
    "                D2_class_frequency = {}\n",
    "                for key, value in class_frequency.items():\n",
    "                    if key in D1_class_frequency:\n",
    "                        D2_class_frequency[key] = value - D1_class_frequency[key]\n",
    "                    else:\n",
    "                        D2_class_frequency[key] = value\n",
    "\n",
    "                for key, v in D1_class_frequency.items():\n",
    "                    g_d1 -= (v/float(D1))**2\n",
    "                for key, v in D2_class_frequency.items():\n",
    "                    g_d2 -= (v/float(D2))**2\n",
    "                g = g_d1*D1/D + g_d2*D2/D\n",
    "                if g < m1:\n",
    "                    m1 = g\n",
    "                    Xa_value = split[index]\n",
    "                elif m1 < g < m2:\n",
    "                    m2 = g\n",
    "            return [m1, Xa_value]\n",
    "\n",
    "        # discrete feature_values\n",
    "        except TypeError:\n",
    "            length = len(njk)\n",
    "            feature_values = list(njk.keys())\n",
    "            right = None\n",
    "            if length > 10:  # too many discrete feature values\n",
    "                for j, k in njk.items():\n",
    "                    D1 = sum(k.values())\n",
    "                    D2 = D - D1\n",
    "                    g_d1 = 1\n",
    "                    g_d2 = 1\n",
    "\n",
    "                    D2_class_frequency = {}\n",
    "                    for key, value in class_frequency.items():\n",
    "                        if key in k:\n",
    "                            D2_class_frequency[key] = value - k[key]\n",
    "                        else:\n",
    "                            D2_class_frequency[key] = value\n",
    "\n",
    "                    for key, v in k.items():\n",
    "                        g_d1 -= (v/D1)**2\n",
    "\n",
    "                    if D2 != 0:\n",
    "                        for key, v in D2_class_frequency.items():\n",
    "                            g_d2 -= (v/D2)**2\n",
    "                    g = g_d1*D1/D + g_d2*D2/D\n",
    "                    if g < m1:\n",
    "                        m1 = g\n",
    "                        Xa_value = j\n",
    "                    elif m1 < g < m2:\n",
    "                        m2 = g\n",
    "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
    "\n",
    "            else:  # fewer discrete feature values\n",
    "                comb = self.select_combinations(feature_values)\n",
    "                for i in comb:\n",
    "                    left = list(i)\n",
    "                    D1 = 0\n",
    "                    D2 = 0\n",
    "                    D1_class_frequency = {key:0 for key in class_frequency.keys()}\n",
    "                    D2_class_frequency = {key:0 for key in class_frequency.keys()}\n",
    "                    for j,k in njk.items():\n",
    "                        for key, value in class_frequency.items():\n",
    "                            if j in left:\n",
    "                                if key in k:\n",
    "                                    D1 += sum(k.values())\n",
    "                                    D1_class_frequency[key] += k[key]\n",
    "                            else:\n",
    "                                if key in k:\n",
    "                                    D2 += sum(k.values())\n",
    "                                    D2_class_frequency[key] += k[key]\n",
    "                    g_d1 = 1\n",
    "                    g_d2 = 1\n",
    "                    for key, v in D1_class_frequency.items():\n",
    "                        g_d1 -= (v/D1)**2\n",
    "                    for key, v in D2_class_frequency.items():\n",
    "                        g_d2 -= (v/D1)**2\n",
    "                    g = g_d1*D1/D + g_d2*D2/D\n",
    "                    if g < m1:\n",
    "                        m1 = g\n",
    "                        Xa_value = left\n",
    "                    elif m1 < g < m2:\n",
    "                        m2 = g\n",
    "                right = list(np.setdiff1d(feature_values, Xa_value))\n",
    "            return [m1, [Xa_value, right]]\n",
    "\n",
    "    def select_combinations(self, feature_values):\n",
    "        combination = []\n",
    "        e = len(feature_values)\n",
    "        if e % 2 == 0:\n",
    "            end = int(e/2)\n",
    "            for i in range(1, end+1):\n",
    "                if i == end:\n",
    "                    cmb = list(combinations(feature_values, i))\n",
    "                    enough = int(len(cmb)/2)\n",
    "                    combination.extend(cmb[:enough])\n",
    "                else:\n",
    "                    combination.extend(combinations(feature_values, i))\n",
    "        else:\n",
    "            end = int((e-1)/2)\n",
    "            for i in range(1, end+1):\n",
    "                combination.extend(combinations(feature_values, i))\n",
    "\n",
    "        return combination\n",
    "\n",
    "# very fast decision tree class, i.e. hoeffding tree\n",
    "class vfdt:\n",
    "    # parameters\n",
    "    # feature_values  # number of values of each feature # dict\n",
    "    # feature_values = {feature:[unique values list]}\n",
    "    # delta   # used for hoeffding bound\n",
    "    # tau  # used to deal with ties\n",
    "    # nmin  # used to limit the G computations\n",
    "\n",
    "    def __init__(self, feature_values, delta=0.05, nmin=50, tau=0.1):\n",
    "        self.feature_values = feature_values\n",
    "        self.delta = delta\n",
    "        self.nmin = nmin\n",
    "        self.tau = tau\n",
    "        features = list(feature_values.keys())\n",
    "        self.root = vfdt_node(features)\n",
    "        self.n_examples_processed = 0\n",
    "\n",
    "    # update the tree by adding training example\n",
    "    def update(self, example):\n",
    "        self.n_examples_processed += 1\n",
    "        node = self.root.sort_example(example)\n",
    "        node.update_stats(example)\n",
    "\n",
    "        result = node.splittable(self.delta, self.nmin, self.tau)\n",
    "        if result != None:\n",
    "            feature = result[0]\n",
    "            value = result[1]\n",
    "            #print('SPLIT')\n",
    "            #print(feature, value)\n",
    "            self.node_split(node, feature)\n",
    "            node.split_value = value\n",
    "\n",
    "    # split node, produce children\n",
    "    def node_split(self, node, split_feature):\n",
    "        features = node.possible_split_features\n",
    "        # replace deleted split feature with None\n",
    "        # new_features = [None if f == split_feature else f for f in features]\n",
    "        left = vfdt_node(features)\n",
    "        right = vfdt_node(features)\n",
    "        node.add_children(split_feature, left, right)\n",
    "\n",
    "    # predict test example's classification\n",
    "    def predict(self, example):\n",
    "        leaf = self.root.sort_example(example)\n",
    "        prediction = leaf.most_frequent()\n",
    "        return(prediction)\n",
    "\n",
    "    # accuracy of a test set\n",
    "    def accuracy(self, examples):\n",
    "        correct = 0\n",
    "        for ex in examples:\n",
    "            if (self.predict(ex) == ex[-1]):\n",
    "                correct +=1\n",
    "        return(float(correct) / len(examples))\n",
    "\n",
    "    def print_tree(self, node):\n",
    "        if node.is_leaf():\n",
    "            print('Leaf')\n",
    "        else:\n",
    "            print(node.split_feature)\n",
    "            self.print_tree(node.left_child)\n",
    "            self.print_tree(node.right_child)\n",
    "\n",
    "def test_run():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # bank.csv whole data size: 4521\n",
    "    # if more than 4521, it revert back to 4521\n",
    "    rows = 4521\n",
    "    # n_training = int(0.8 * rows)\n",
    "    # read_csv has parameter nrows=n that read the first n rows\n",
    "    '''skiprows=1, index_col=0,'''\n",
    "    df = pd.read_csv('./dataset/bank.csv', nrows=rows, header=0, sep=';')\n",
    "    title = list(df.columns.values)\n",
    "    features = title[:-1]\n",
    "\n",
    "    ''' change month string to int '''\n",
    "    import calendar\n",
    "    d = dict((v.lower(),k) for k,v in enumerate(calendar.month_abbr))\n",
    "    df.month = df.month.map(d)\n",
    "    #print(df.head(10))\n",
    "\n",
    "    # unique values for each feature to use in VFDT\n",
    "    feature_values = {f:None for f in features}\n",
    "    for f in features:\n",
    "        feature_values[f] = df[f].unique()\n",
    "\n",
    "    # convert df to data examples\n",
    "    array = df.head(4000).values\n",
    "    set1 = []\n",
    "    set2 = []\n",
    "    set3 = []\n",
    "    possible_split_features = title[:-1]\n",
    "    count = 0\n",
    "    for i in range(len(array)):\n",
    "        count += 1\n",
    "        if (count <= 1000):\n",
    "            set1.append(array[i])\n",
    "        elif (count > 1000 and count <= 2000):\n",
    "            set2.append(array[i])\n",
    "        else:\n",
    "            set3.append(array[i])\n",
    "    # to simulate continous training, modify the tree for each training set\n",
    "    examples = [set1, set2, set3]\n",
    "\n",
    "    # test set is different from training set\n",
    "    n_test = 500\n",
    "    test_set = df.tail(n_test).values\n",
    "    test = []\n",
    "    for i in range(len(test_set)):\n",
    "        test.append(test_set[i])\n",
    "\n",
    "    # heoffding bound parameter delta: with 1 - delta probability\n",
    "    # the true mean is at least r - gamma\n",
    "    # vfdt parameter nmin: test split if new sample size > nmin\n",
    "\n",
    "    tree = vfdt(feature_values, delta=0.05, nmin=100, tau=0.05)\n",
    "    print('Total data size: ', rows)\n",
    "    print('Test set (tail): ', len(test_set))\n",
    "    n = 0\n",
    "    for training_set in examples:\n",
    "        n += len(training_set)\n",
    "        for ex in training_set:\n",
    "            tree.update(ex)\n",
    "        print('Training set:', n, end=', ')\n",
    "        print('ACCURACY: %.4f' % tree.accuracy(test))\n",
    "\n",
    "    tree.print_tree(tree.root)\n",
    "    print(\"--- Running time: %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343986f8465ea7f501665066340a6e4a7b639b52c302955116ce941ba04487e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
