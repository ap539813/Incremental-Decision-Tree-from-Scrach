{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we need to do is import the required libraries\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decisiond_tree():\n",
    "    d_tree = {}\n",
    "\n",
    "    def learn(self, training_data, features, target):\n",
    "        # def train(training_data, features, target):\n",
    "      training_data = [item for item in training_data]\n",
    "      vals = [record[features.index(target)] for record in training_data]\n",
    "      frequency = {key:0 for key in [item[-1] for item in training_data]}\n",
    "      idx = features.index(target)\n",
    "      for tup in training_data:\n",
    "          frequency[tup[idx]] += 1\n",
    "      max, major = 0, 0\n",
    "      for key in frequency.keys():\n",
    "          if frequency[key]>max:\n",
    "              max = frequency[key]\n",
    "              major = key\n",
    "\n",
    "      if not training_data:\n",
    "          return major\n",
    "      elif (len(features) - 1) <= 0:\n",
    "          return major\n",
    "      elif vals.count(vals[0]) == len(vals):\n",
    "          return vals[0]\n",
    "      else:\n",
    "          best = features[0]\n",
    "          minimum_gini = 10000;\n",
    "\n",
    "          for attr in features:\n",
    "              new_gini_index = get_gini(features, training_data, attr, target)\n",
    "              if new_gini_index<minimum_gini:\n",
    "                  minimum_gini = new_gini_index\n",
    "                  best = attr\n",
    "          d_tree = {best:{}}\n",
    "\n",
    "          idx = features.index(best)\n",
    "          got_vals = []\n",
    "          for entry in training_data:\n",
    "              if entry[idx] not in got_vals:\n",
    "                  got_vals.append(float(entry[idx]))\n",
    "          for val in got_vals: #get_values(data, features, best):\n",
    "              new_data = [[entry[i] for i in range(0, len(entry)) if i != idx] for entry in training_data if entry[idx] == val]\n",
    "              newAttr = features[:]\n",
    "              newAttr.remove(best)\n",
    "              subtree = self.learn(new_data, newAttr, target)\n",
    "              d_tree[best][val] = subtree\n",
    "      return d_tree\n",
    "      \n",
    "    def classify(self, test_instance, features):\n",
    "        # Followinf node is to make sure that we create an entity as node\n",
    "        # the node will store the name of a node in the tree and its children\n",
    "        class Node():\n",
    "          def __init__(self, val, dictionary):\n",
    "              self.value = val\n",
    "              if (isinstance(dictionary, dict)):\n",
    "                  self.children = dictionary.keys()\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        # create a copy of d_tree in the varable temp_dict\n",
    "        temp_dict = self.d_tree.copy()\n",
    "\n",
    "        # Traverse the tree untill reaching the leaf node\n",
    "        while(isinstance(temp_dict, dict)):\n",
    "          # get to the root node of the currect subtree\n",
    "          root = Node(list(temp_dict.keys())[0], temp_dict[list(temp_dict.keys())[0]])\n",
    "          # Store the children of the current node in the temp_dict variable\n",
    "          temp_dict = temp_dict[list(temp_dict.keys())[0]]\n",
    "          # store the idx of the attribute of the current node and store it in idx variable\n",
    "          idx = features.index(root.value)\n",
    "          # Pick the value from the test tuple for the variable location stored in idx variable\n",
    "          value = test_instance[idx]\n",
    "          # store the keys of the \n",
    "          if value in temp_dict.keys():\n",
    "            child = Node(value, temp_dict[value])\n",
    "            result = temp_dict[value]\n",
    "            temp_dict = temp_dict[value]\n",
    "          else:\n",
    "            result = None\n",
    "            break\n",
    "        return result\n",
    "\n",
    "    def gini_target(self, features, data, target_attributes):\n",
    "        # Set up a dictionary to store the no of occurences of the target attribute\n",
    "        freq = {}\n",
    "        # Set the sum of probabilitys\n",
    "        sum_p2 = 0.0\n",
    "        i = features.index(target_attributes) - 1\n",
    "        for entry in data:\n",
    "            if (entry[i] in freq.keys()):\n",
    "                freq[entry[i]] += 1.0\n",
    "            else:\n",
    "                freq[entry[i]]  = 1.0\n",
    "        for freq in freq.values():\n",
    "            sum_p2 += (freq/len(data))**2\n",
    "        gini = 1 - sum_p2\n",
    "        return gini\n",
    "\n",
    "    def get_gini(self, features, data, attr, target_attributes):\n",
    "        # This function will help to calculate the Gini index of the part of data\n",
    "        # Using this function we will calculate the Gini index for each split made by each attribute\n",
    "        var_occur_n = {}\n",
    "\n",
    "        # Subset entropy variable will contain the sum of entropy of each and every split of the attribute\n",
    "        gini_sum = 0.0\n",
    "        # store the idx of the current attribute column\n",
    "        i = features.index(attr)\n",
    "\n",
    "        # Iterating over the the data provided we will canculate the frequency of occurence of \n",
    "        # the unique elements of the attribute\n",
    "        for entry in data:\n",
    "            # for each entry in the data update their frequency\n",
    "            # If the entry is listed as a key then increment the frequency\n",
    "            # On the other hand if the entry is not listed then add it as a new element\n",
    "            if (entry[i] in var_occur_n.keys()):\n",
    "                var_occur_n[entry[i]] += 1.0\n",
    "            else:\n",
    "                var_occur_n[entry[i]]  = 1.0\n",
    "\n",
    "        # For each key in the dictionary canculate the entropy of corresponding subset of data\n",
    "        # then add those entropy values in the subset_entropy\n",
    "        for val in var_occur_n.keys():\n",
    "            Probability_val        = var_occur_n[val] / sum(var_occur_n.values())\n",
    "            dataSubset     = [entry for entry in data if entry[i] == val]\n",
    "            gini_sum += Probability_val * gini_target(features, dataSubset, target_attributes)\n",
    "\n",
    "        return gini_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "343986f8465ea7f501665066340a6e4a7b639b52c302955116ce941ba04487e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
